{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPddMvLVBlchY0+filmY4K2",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Arishiine/Colab-Fraud-Project/blob/main/Untitled3.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "h6MTq6nvmf1X",
        "outputId": "1107a311-cad8-4595-e8b7-8f54d3ad7305"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dataset loaded successfully.\n",
            "Dataset columns: ['Transaction_ID', 'User_ID', 'Transaction_Amount', 'Transaction_Type', 'Timestamp', 'Account_Balance', 'Device_Type', 'Location', 'Merchant_Category', 'IP_Address_Flag', 'Previous_Fraudulent_Activity', 'Daily_Transaction_Count', 'Avg_Transaction_Amount_7d', 'Failed_Transaction_Count_7d', 'Card_Type', 'Card_Age', 'Transaction_Distance', 'Authentication_Method', 'Risk_Score', 'Is_Weekend', 'Fraud_Label']\n",
            "Numerical features: ['Transaction_Amount', 'Account_Balance', 'IP_Address_Flag', 'Previous_Fraudulent_Activity', 'Daily_Transaction_Count', 'Avg_Transaction_Amount_7d', 'Failed_Transaction_Count_7d', 'Card_Age', 'Transaction_Distance', 'Risk_Score', 'Is_Weekend']\n",
            "Categorical features: ['Transaction_ID', 'User_ID', 'Transaction_Type', 'Timestamp', 'Device_Type', 'Location', 'Merchant_Category', 'Card_Type', 'Authentication_Method']\n",
            "Training Logistic Regression...\n",
            "Logistic Regression training complete.\n",
            "Training Random Forest...\n",
            "Random Forest training complete.\n",
            "Training LightGBM...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[LightGBM] [Info] Number of positive: 12050, number of negative: 25450\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.005049 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 1595\n",
            "[LightGBM] [Info] Number of data points in the train set: 37500, number of used features: 36\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.321333 -> initscore=-0.747651\n",
            "[LightGBM] [Info] Start training from score -0.747651\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "LightGBM training complete.\n",
            "\n",
            "--- Model Evaluation ---\n",
            "\n",
            "Evaluating Logistic Regression:\n",
            "Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.87      0.82      0.84      8483\n",
            "           1       0.66      0.73      0.69      4017\n",
            "\n",
            "    accuracy                           0.79     12500\n",
            "   macro avg       0.76      0.78      0.77     12500\n",
            "weighted avg       0.80      0.79      0.79     12500\n",
            "\n",
            "Confusion Matrix:\n",
            "[[6969 1514]\n",
            " [1086 2931]]\n",
            "ROC AUC Score: 0.8878\n",
            "\n",
            "Evaluating Random Forest:\n",
            "Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       1.00      1.00      1.00      8483\n",
            "           1       1.00      1.00      1.00      4017\n",
            "\n",
            "    accuracy                           1.00     12500\n",
            "   macro avg       1.00      1.00      1.00     12500\n",
            "weighted avg       1.00      1.00      1.00     12500\n",
            "\n",
            "Confusion Matrix:\n",
            "[[8483    0]\n",
            " [   0 4017]]\n",
            "ROC AUC Score: 1.0000\n",
            "\n",
            "Evaluating LightGBM:\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       1.00      1.00      1.00      8483\n",
            "           1       1.00      1.00      1.00      4017\n",
            "\n",
            "    accuracy                           1.00     12500\n",
            "   macro avg       1.00      1.00      1.00     12500\n",
            "weighted avg       1.00      1.00      1.00     12500\n",
            "\n",
            "Confusion Matrix:\n",
            "[[8480    3]\n",
            " [   9 4008]]\n",
            "ROC AUC Score: 1.0000\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n",
            "  warnings.warn(\n"
          ]
        }
      ],
      "source": [
        "# prompt: build 3 models to predict fraud\n",
        "\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
        "from sklearn.compose import ColumnTransformer\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "import lightgbm as lgb\n",
        "from sklearn.metrics import classification_report, roc_auc_score, confusion_matrix\n",
        "\n",
        "# --- 1. Data Loading and Preprocessing ---\n",
        "\n",
        "# Load the data from the provided CSV file\n",
        "try:\n",
        "    df = pd.read_csv('/content/synthetic_fraud_dataset.csv')\n",
        "    print(\"Dataset loaded successfully.\")\n",
        "except FileNotFoundError:\n",
        "    print(\"Error: '/content/synthetic_fraud_dataset.csv' not found. Please upload your data file.\")\n",
        "    # Exit or handle the error appropriately if the file is not found\n",
        "    exit() # Or raise an exception, or create dummy data as before if that's desired\n",
        "\n",
        "\n",
        "# Inspect the columns to identify the target column and feature types\n",
        "print(\"Dataset columns:\", df.columns.tolist())\n",
        "\n",
        "# Assuming the target column is named 'Fraud_Label' based on the dataset inspection\n",
        "target_column = 'Fraud_Label'\n",
        "if target_column not in df.columns:\n",
        "    print(f\"Error: Target column '{target_column}' not found in the dataset.\")\n",
        "    # Attempt to guess the target column or exit\n",
        "    # For now, let's assume 'Fraud_Label' is the correct one and exit if not found\n",
        "    exit()\n",
        "\n",
        "\n",
        "# Separate features (X) and target (y)\n",
        "X = df.drop(target_column, axis=1)\n",
        "y = df[target_column]\n",
        "\n",
        "# Identify numerical and categorical features (adjust based on your dataset)\n",
        "# This part will need adjustment based on the actual column types in your CSV\n",
        "numerical_features = X.select_dtypes(include=['int64', 'float64']).columns\n",
        "categorical_features = X.select_dtypes(include=['object', 'category']).columns\n",
        "\n",
        "print(\"Numerical features:\", numerical_features.tolist())\n",
        "print(\"Categorical features:\", categorical_features.tolist())\n",
        "\n",
        "\n",
        "# Create preprocessing pipelines for numerical and categorical features\n",
        "numerical_transformer = StandardScaler()\n",
        "categorical_transformer = OneHotEncoder(handle_unknown='ignore')\n",
        "\n",
        "# Create a column transformer to apply the transformations\n",
        "# Only include transformers if there are features of that type\n",
        "transformers = []\n",
        "if len(numerical_features) > 0:\n",
        "    transformers.append(('num', numerical_transformer, numerical_features))\n",
        "if len(categorical_features) > 0:\n",
        "    transformers.append(('cat', categorical_transformer, categorical_features))\n",
        "\n",
        "if len(transformers) == 0:\n",
        "    print(\"Error: No numerical or categorical features found for preprocessing.\")\n",
        "    exit()\n",
        "\n",
        "preprocessor = ColumnTransformer(transformers=transformers)\n",
        "\n",
        "\n",
        "# Split data into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state=42, stratify=y) # Stratify for imbalanced data\n",
        "\n",
        "# --- 2. Model Selection and Training ---\n",
        "\n",
        "# Model 1: Logistic Regression\n",
        "model_lr = Pipeline(steps=[('preprocessor', preprocessor),\n",
        "                           ('classifier', LogisticRegression(solver='liblinear', random_state=42, class_weight='balanced'))]) # Use class_weight='balanced' for imbalanced data\n",
        "\n",
        "print(\"Training Logistic Regression...\")\n",
        "model_lr.fit(X_train, y_train)\n",
        "print(\"Logistic Regression training complete.\")\n",
        "\n",
        "# Model 2: Random Forest\n",
        "model_rf = Pipeline(steps=[('preprocessor', preprocessor),\n",
        "                           ('classifier', RandomForestClassifier(n_estimators=100, random_state=42, class_weight='balanced'))]) # Use class_weight='balanced' for imbalanced data\n",
        "\n",
        "print(\"Training Random Forest...\")\n",
        "model_rf.fit(X_train, y_train)\n",
        "print(\"Random Forest training complete.\")\n",
        "\n",
        "# Model 3: LightGBM\n",
        "# LightGBM can handle categorical features directly if they are encoded as integers\n",
        "# For simplicity here, we'll let the preprocessor handle encoding.\n",
        "model_lgb = Pipeline(steps=[('preprocessor', preprocessor),\n",
        "                            ('classifier', lgb.LGBMClassifier(random_state=42, objective='binary', metric='auc', is_unbalance=True))]) # Use is_unbalance=True for imbalanced data\n",
        "\n",
        "print(\"Training LightGBM...\")\n",
        "model_lgb.fit(X_train, y_train)\n",
        "print(\"LightGBM training complete.\")\n",
        "\n",
        "# --- 4. Evaluation ---\n",
        "\n",
        "print(\"\\n--- Model Evaluation ---\")\n",
        "\n",
        "models = {\n",
        "    \"Logistic Regression\": model_lr,\n",
        "    \"Random Forest\": model_rf,\n",
        "    \"LightGBM\": model_lgb\n",
        "}\n",
        "\n",
        "for name, model in models.items():\n",
        "    print(f\"\\nEvaluating {name}:\")\n",
        "\n",
        "    y_pred = model.predict(X_test)\n",
        "    y_prob = model.predict_proba(X_test)[:, 1] # Probability of the positive class\n",
        "\n",
        "    print(\"Classification Report:\")\n",
        "    print(classification_report(y_test, y_pred))\n",
        "\n",
        "    print(\"Confusion Matrix:\")\n",
        "    print(confusion_matrix(y_test, y_pred))\n",
        "\n",
        "    auc_score = roc_auc_score(y_test, y_prob)\n",
        "    print(f\"ROC AUC Score: {auc_score:.4f}\")\n",
        "\n",
        "# --- 5. Comparison (Implicit in the evaluation output) ---\n",
        "# You can now compare the metrics printed for each model to see which performs best.\n",
        "# For fraud detection, Recall (the ability to find all positive samples) and\n",
        "# Precision (the accuracy of positive predictions) are often key, along with AUC."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "80701d27"
      },
      "source": [
        "# Task\n",
        "Use the data-clean, feature engineer, and train three models (Logistic Regression, Random Forest, and LightGBM) on the dataset \"fraud_detection_dataset.csv\". Predict fraud and write a Python application for deployment."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9a78b3ac"
      },
      "source": [
        "## Data cleaning and feature engineering\n",
        "\n",
        "### Subtask:\n",
        "Address the `KeyError` by correcting the target column name and perform any necessary data cleaning and feature engineering steps based on the actual dataset columns.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7ab66e96"
      },
      "source": [
        "**Reasoning**:\n",
        "The traceback indicates a KeyError because the column 'is_fraud' was not found in the DataFrame. I need to inspect the DataFrame `df` to identify the correct target column name and the feature types. Since the file was not found, a dummy dataset was created, which does not have a column named 'is_fraud'. The error occurs when trying to drop 'is_fraud'. I need to examine the dummy dataset's columns and correct the code to reflect the dummy dataset's structure.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "20282407"
      },
      "source": [
        "## Model Saving\n",
        "\n",
        "Now I will save the trained models and the preprocessor using `joblib` so they can be used later for making predictions on new data without retraining."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "73a3be80",
        "outputId": "7666ac3d-d7b0-44a3-d917-589e38b6fcbd"
      },
      "source": [
        "import joblib\n",
        "import os\n",
        "\n",
        "# Create a directory to save the models if it doesn't exist\n",
        "if not os.path.exists('saved_models'):\n",
        "    os.makedirs('saved_models')\n",
        "\n",
        "# Save the preprocessor\n",
        "joblib.dump(preprocessor, 'saved_models/preprocessor.joblib')\n",
        "\n",
        "# Save each trained model\n",
        "joblib.dump(model_lr, 'saved_models/logistic_regression_model.joblib')\n",
        "joblib.dump(model_rf, 'saved_models/random_forest_model.joblib')\n",
        "joblib.dump(model_lgb, 'saved_models/lightgbm_model.joblib')\n",
        "\n",
        "print(\"Models and preprocessor saved to 'saved_models' directory.\")"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Models and preprocessor saved to 'saved_models' directory.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d69735a9"
      },
      "source": [
        "## Streamlit Application for Deployment\n",
        "\n",
        "Here is a Python script for a basic Streamlit application that loads the saved models and preprocessor and allows you to input transaction data to get fraud predictions.\n",
        "\n",
        "**To run this Streamlit app:**\n",
        "\n",
        "1.  Make sure you have Streamlit installed (`pip install streamlit`).\n",
        "2.  Save the code below as a Python file (e.g., `fraud_app.py`) in the same directory where you saved the `saved_models` folder.\n",
        "3.  Open your terminal or command prompt, navigate to that directory, and run the command: `streamlit run fraud_app.py`"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 106
        },
        "id": "da2d872d",
        "outputId": "ad64b7fc-846a-4492-e086-dae96af97b32"
      },
      "source": [
        "import streamlit as st\n",
        "import pandas as pd\n",
        "import joblib\n",
        "import os\n",
        "\n",
        "# Load the saved preprocessor and models\n",
        "@st.cache_resource # Cache the resource to avoid reloading on every rerun\n",
        "def load_models():\n",
        "    try:\n",
        "        preprocessor = joblib.load('saved_models/preprocessor.joblib')\n",
        "        model_lr = joblib.load('saved_models/logistic_regression_model.joblib')\n",
        "        model_rf = joblib.load('saved_models/random_forest_model.joblib')\n",
        "        model_lgb = joblib.load('saved_models/lightgbm_model.joblib')\n",
        "        st.success(\"Models and preprocessor loaded successfully.\")\n",
        "        return preprocessor, model_lr, model_rf, model_lgb\n",
        "    except FileNotFoundError:\n",
        "        st.error(\"Error: Saved models or preprocessor not found. Please ensure 'saved_models' directory and its contents exist.\")\n",
        "        return None, None, None, None\n",
        "\n",
        "preprocessor, model_lr, model_rf, model_lgb = load_models()\n",
        "\n",
        "# Set up the Streamlit app title and description\n",
        "st.title(\"Fraud Detection Application\")\n",
        "st.write(\"Upload a CSV file with transaction data or enter details manually to predict fraud.\")\n",
        "\n",
        "# Option to upload a CSV file\n",
        "st.header(\"Upload Transaction Data (CSV)\")\n",
        "uploaded_file = st.file_uploader(\"Choose a CSV file\", type=\"csv\")\n",
        "\n",
        "if uploaded_file is not None:\n",
        "    try:\n",
        "        new_data_df = pd.read_csv(uploaded_file)\n",
        "        st.write(\"Uploaded Data:\")\n",
        "        st.dataframe(new_data_df)\n",
        "\n",
        "        # Select model for prediction\n",
        "        model_choice_upload = st.selectbox(\n",
        "            \"Select Model for Prediction (Uploaded Data):\",\n",
        "            (\"Random Forest\", \"Logistic Regression\", \"LightGBM\")\n",
        "        )\n",
        "\n",
        "        if st.button(\"Predict Fraud (Uploaded Data)\"):\n",
        "            if preprocessor and model_lr and model_rf and model_lgb:\n",
        "                # Make predictions\n",
        "                if model_choice_upload == \"Logistic Regression\":\n",
        "                    model = model_lr\n",
        "                elif model_choice_upload == \"Random Forest\":\n",
        "                    model = model_rf\n",
        "                else:\n",
        "                    model = model_lgb\n",
        "\n",
        "                try:\n",
        "                    # Ensure the uploaded data has the same columns as the training data (excluding target)\n",
        "                    # You might need more robust column handling here depending on your data\n",
        "                    # For simplicity, assuming column order and names match\n",
        "                    processed_data = preprocessor.transform(new_data_df)\n",
        "                    predictions = model.predict(processed_data)\n",
        "                    probabilities = model.predict_proba(processed_data)[:, 1]\n",
        "\n",
        "                    results_df = new_data_df.copy()\n",
        "                    results_df['Predicted_Fraud'] = predictions\n",
        "                    results_df['Probability_Fraud'] = probabilities\n",
        "\n",
        "                    st.write(\"\\nPrediction Results (Uploaded Data):\")\n",
        "                    st.dataframe(results_df)\n",
        "\n",
        "                except Exception as e:\n",
        "                    st.error(f\"Error during prediction: {e}\")\n",
        "                    st.write(\"Please ensure the uploaded CSV has the correct format and columns matching the training data.\")\n",
        "            else:\n",
        "                st.warning(\"Models not loaded. Please check the 'saved_models' directory.\")\n",
        "\n",
        "# Option to enter data manually (example for a few key features)\n",
        "st.header(\"Enter Transaction Details Manually\")\n",
        "\n",
        "# You will need to add input fields for each feature your model expects\n",
        "# This is a simplified example with only a few features\n",
        "# Get the list of expected features from the preprocessor\n",
        "if preprocessor:\n",
        "    # Assuming the preprocessor's fitted transformers can give us feature names\n",
        "    # This might require inspecting the fitted preprocessor object based on its type\n",
        "    # For ColumnTransformer, you can access the columns used by each transformer\n",
        "    try:\n",
        "        # This is a simplified way to get feature names, may need adjustment\n",
        "        # You may need to access the feature names differently based on your specific preprocessor setup\n",
        "        # For example, if using ColumnTransformer with named transformers, you might need to iterate\n",
        "        # through the transformers and get their feature names.\n",
        "        # As a temporary fix to unblock, let's define a dummy numerical_features list\n",
        "        # assuming the original dataset columns are still available in the environment\n",
        "        # In a real Streamlit app, you'd need a more robust way to get feature names from the loaded preprocessor\n",
        "        try:\n",
        "          # Attempt to get feature names from the preprocessor if it's a ColumnTransformer\n",
        "          if isinstance(preprocessor, ColumnTransformer):\n",
        "              input_features = []\n",
        "              for name, transformer, features in preprocessor.transformers_:\n",
        "                  if hasattr(transformer, 'get_feature_names_out'):\n",
        "                      input_features.extend(transformer.get_feature_names_out(features))\n",
        "                  else:\n",
        "                      # Fallback if the transformer doesn't have get_feature_names_out\n",
        "                      input_features.extend(features)\n",
        "          else:\n",
        "             st.warning(\"Preprocessor type not recognized for automatic feature name extraction. Please manually define input fields.\")\n",
        "             input_features = [] # Fallback\n",
        "        except Exception as e:\n",
        "             st.warning(f\"Could not automatically get feature names from preprocessor: {e}\")\n",
        "             st.write(\"Please manually define the input fields for your features.\")\n",
        "             input_features = [] # Fallback to empty list if feature names can't be extracted\n",
        "\n",
        "        manual_input_data = {}\n",
        "        st.write(\"Please enter values for the following features:\")\n",
        "        # Create input fields dynamically based on identified features\n",
        "        for feature in input_features:\n",
        "            # You would need to add more sophisticated input types based on feature dtype\n",
        "            manual_input_data[feature] = st.text_input(f\"{feature}:\", \"\")\n",
        "\n",
        "        # Add a button to predict with manual input\n",
        "        if st.button(\"Predict Fraud (Manual Input)\"):\n",
        "            if preprocessor and model_lr and model_rf and model_lgb:\n",
        "                # Convert manual input to a DataFrame\n",
        "                try:\n",
        "                    # Convert input values to appropriate types if necessary\n",
        "                    # This is a basic conversion, you might need more specific handling\n",
        "                    manual_input_processed = {}\n",
        "                    for feature, value in manual_input_data.items():\n",
        "                        # Attempt to infer type or use a default (e.g., string)\n",
        "                        try:\n",
        "                            # Try converting to float\n",
        "                            manual_input_processed[feature] = [float(value)]\n",
        "                        except ValueError:\n",
        "                            # If conversion to float fails, keep as string\n",
        "                            manual_input_processed[feature] = [value]\n",
        "\n",
        "                    manual_input_df = pd.DataFrame(manual_input_processed)\n",
        "\n",
        "                    st.write(\"Manual Input Data:\")\n",
        "                    st.dataframe(manual_input_df)\n",
        "\n",
        "                    # Select model for prediction\n",
        "                    model_choice_manual = st.selectbox(\n",
        "                        \"Select Model for Prediction (Manual Input):\",\n",
        "                        (\"Random Forest\", \"Logistic Regression\", \"LightGBM\"),\n",
        "                        key='manual_model_choice' # Add a unique key\n",
        "                    )\n",
        "\n",
        "                    # Make predictions\n",
        "                    if model_choice_manual == \"Logistic Regression\":\n",
        "                        model = model_lr\n",
        "                    elif model_choice_manual == \"Random Forest\":\n",
        "                        model = model_rf\n",
        "                    else:\n",
        "                        model = model_lgb\n",
        "\n",
        "                    try:\n",
        "                         processed_data_manual = preprocessor.transform(manual_input_df)\n",
        "                         predictions_manual = model.predict(processed_data_manual)\n",
        "                         probabilities_manual = model.predict_proba(processed_data_manual)[:, 1]\n",
        "\n",
        "                         st.write(\"\\nPrediction Result (Manual Input):\")\n",
        "                         st.write(f\"Predicted Fraud: {'Yes' if predictions_manual[0] == 1 else 'No'}\")\n",
        "                         st.write(f\"Probability of Fraud: {probabilities_manual[0]:.4f}\")\n",
        "\n",
        "                    except Exception as e:\n",
        "                         st.error(f\"Error during prediction: {e}\")\n",
        "                         st.write(\"Please ensure the manual input data matches the expected format and features.\")\n",
        "\n",
        "                except Exception as e:\n",
        "                    st.error(f\"Error creating DataFrame from manual input: {e}\")\n",
        "            else:\n",
        "                 st.warning(\"Models not loaded. Please check the 'saved_models' directory.\")\n",
        "\n",
        "else:\n",
        "    st.info(\"Models are being loaded...\")\n",
        "\n",
        "st.markdown(\"---\")\n",
        "st.write(\"Developed by Your Name/Team\") # Optional: Add your name or team name"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "error",
          "ename": "SyntaxError",
          "evalue": "expected 'except' or 'finally' block (ipython-input-7-2505689984.py, line 74)",
          "traceback": [
            "\u001b[0;36m  File \u001b[0;32m\"/tmp/ipython-input-7-2505689984.py\"\u001b[0;36m, line \u001b[0;32m74\u001b[0m\n\u001b[0;31m    st.header(\"Enter Transaction Details Manually\")\u001b[0m\n\u001b[0m                                                   ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m expected 'except' or 'finally' block\n"
          ]
        }
      ]
    }
  ]
}